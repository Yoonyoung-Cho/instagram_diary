{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import json\n",
    "import re \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from langchain import hub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate\n",
    "\n",
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()\n",
    "\n",
    "# Load data \n",
    "with open (\"instagram_diary/content/posts_1.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame()\n",
    "for i in range(len(data)):\n",
    "    df = pd.concat([df,pd.DataFrame(data[i])])\n",
    "df = df.map(lambda x: x.encode('latin-1').decode('utf-8') if isinstance(x, str) else x)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#  전처리 \n",
    "df.loc[df.title.isna(),'creation_timestamp']= [x.get('creation_timestamp') for x in df.loc[df.title.isna()]['media']]\n",
    "df.loc[df.title.isna(),'title']= [x.get('title').encode('latin-1').decode('utf-8') for x in df.loc[df.title.isna()]['media']]\n",
    "df['uri'] = df.media.map(lambda x: x['uri'])\n",
    "df['dt'] = [datetime.fromtimestamp(x) for x in df['creation_timestamp']]\n",
    "df['year'] = df['dt'].map(lambda x: str(x.year))\n",
    "df['month'] = df['dt'].map(lambda x: x.month)\n",
    "df['day'] = df['dt'].map(lambda x: x.day)\n",
    "df['number'] = df.title.rank(method='dense').astype(int)\n",
    "df_nodup = df[[\"title\", 'year', 'month', 'day', 'number']].drop_duplicates(keep='last')\n",
    "df_nodup.reset_index(drop=True).head()\n",
    "\n",
    "\n",
    "\n",
    "loader = DataFrameLoader(df_nodup, page_content_column=\"title\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "split_docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "vectorstore = FAISS.from_documents(documents=split_docs, embedding=OpenAIEmbeddings())\n",
    "vectorstore.save_local('./db/faiss_openai')\n",
    "# vectorstore = FAISS.from_documents(documents=split_docs, embedding=HuggingFaceBgeEmbeddings())\n",
    "# vectorstore = FAISS.load_local('../db/faiss', HuggingFaceBgeEmbeddings(), allow_dangerous_deserialization=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insta",
   "language": "python",
   "name": "insta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

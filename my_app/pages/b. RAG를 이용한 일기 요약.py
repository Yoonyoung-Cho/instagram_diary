import streamlit as st
import json
import re 
import pandas as pd
from datetime import datetime
from langchain import hub
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma, FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import DataFrameLoader
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain_community.vectorstores import FAISS

from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain.prompts.chat import HumanMessagePromptTemplate

# API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼
from dotenv import load_dotenv

# API í‚¤ ì •ë³´ ë¡œë“œ
load_dotenv()

# Load data 
with open ("../instagram_diary/content/posts_1.json", "r") as f:
    data = json.load(f)
df = pd.DataFrame()
for i in range(len(data)):
    df = pd.concat([df,pd.DataFrame(data[i])])
df = df.map(lambda x: x.encode('latin-1').decode('utf-8') if isinstance(x, str) else x)
df = df.reset_index(drop=True)

#  ì „ì²˜ë¦¬ 
df.loc[df.title.isna(),'creation_timestamp']= [x.get('creation_timestamp') for x in df.loc[df.title.isna()]['media']]
df.loc[df.title.isna(),'title']= [x.get('title').encode('latin-1').decode('utf-8') for x in df.loc[df.title.isna()]['media']]
df['uri'] = df.media.map(lambda x: x['uri'])
df['dt'] = [datetime.fromtimestamp(x) for x in df['creation_timestamp']]
df['year'] = df['dt'].map(lambda x: str(x.year))
df['month'] = df['dt'].map(lambda x: x.month)
df['day'] = df['dt'].map(lambda x: x.day)
df_nodup = df[["title", 'year', 'month', 'day']].drop_duplicates(keep='last')
df_nodup.reset_index(drop=True).head()



loader = DataFrameLoader(df_nodup, page_content_column="title")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)
split_docs = loader.load_and_split(text_splitter=text_splitter)
# vectorstore = FAISS.load_local('../db/faiss', HuggingFaceBgeEmbeddings(), allow_dangerous_deserialization=True)
vectorstore = FAISS.load_local('../db/faiss_openai', OpenAIEmbeddings(), allow_dangerous_deserialization=True)

k = 6
bm25_retriever = BM25Retriever.from_documents(split_docs)
bm25_retriever.k = k
faiss_retriever = vectorstore.as_retriever(search_kwargs={"k": k})
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]
)

# prompt = hub.pull("rlm/rag-prompt")

prompt_txt = """ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ë¹„ì„œì…ë‹ˆë‹¤. 
contextì—ì„œ ì¼ê¸°ì¥ ë‚´ìš©ì´ ì£¼ì–´ì§€ë©´ ê·¸ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ì¹œì ˆí•œ ë§íˆ¬ë¡œ ë‹µë³€í•˜ì„¸ìš”.
ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ë§Œ í•˜ë„ë¡ í•˜ê³ , ë§Œì•½ ë‹µì„ ëª¨ë¥¸ë‹¤ë©´, ê·¸ëƒ¥ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì‹­ì‹œì˜¤. 
ë‹¨ê³„ë³„ë¡œ ìƒê°í•˜ê³  ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. 

Question: {question} 
Context: {context} 
Answer:
"""

prompt=ChatPromptTemplate(input_variables=['context', 'question'],
                #    metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'},
                   messages=[HumanMessagePromptTemplate(
                       prompt=PromptTemplate(input_variables=['context', 'question'], 
                                             template=prompt_txt))])


llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

def format_docs(docs):
    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": ensemble_retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)


st.markdown('# GPTì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš” ğŸ”')
st.markdown('ì¼ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–´ëŠ ë‚  ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆëŠ”ì§€, ìš”ì•½í•´ë“œë¦´ê²Œìš”!')



query = st.text_input("ì•Œê³ ì‹¶ì€ ë‚´ìš©ì— ëŒ€í•´ì„œ ì…ë ¥í•˜ì„¸ìš”.") 


if query : 
    response = rag_chain.invoke(query)
    llm_response = st.text_area("ì¼ê¸°ë‚´ìš©", response, label_visibility='hidden', height=200)

    docs = ensemble_retriever.get_relevant_documents(query)

    docs_index = []
    for i in range(len(docs)):
        docs_index_tmp = df.title.map(lambda x: docs[i].page_content in x)
        docs_df=df[docs_index_tmp]
        docs_index.extend(docs_df.index.to_list())

    data = df.loc[docs_index]

    manuscripts = st.multiselect("ì¼ê¸° ì„ íƒ", data.title.unique(), data.title.unique())
    
    
    paths = data.loc[data.title.isin(manuscripts),'uri']
    paths = [s for s in paths if "jpg" in s] 
        
    

    n = st.number_input("Select Grid Width", 5, 10, 6)

    groups = []
    for i in range(0, len(paths), n):
        groups.append(paths[i:i+n])


    for group in groups:
        cols = st.columns(n)
        for i, image in enumerate(group):
            
            cols[i].image(f"../instagram_diary/{image}")

    txt = data.loc[data.title.isin(manuscripts),'title'].reset_index(drop=True)[0]
    diary_txt = st.text_area("ì¼ê¸°ë‚´ìš©", txt, label_visibility='hidden', height=400)

        